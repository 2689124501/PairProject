{
    "code": 200,
    "message": "OK",
    "data": [
        {
            "paperId": 1,
            "title": "360/spl times/360 mosaics",
            "link": "https://doi.org/10.1109/CVPR.2000.854857",
            "abstracts": "Current mosaicing methods use narrow field of view cameras to acquire image data. This poses problems when computing a complete spherical mosaic. First, a large number of images are needed to capture a sphere. Second, errors in mosaicing make it difficult to complete the spherical mosaic without seams. Third, with a hand-held camera it is hard for the user to ensure complete coverage of the sphere. This paper presents two approaches to spherical mosaicing. The first is to rotate a 360 degree camera about a single axis to capture a sequence of 360 degree strips. The unknown rotations between the strips are estimated and the strips are blended together to obtain a spherical mosaic. The second approach seeks to significantly enhance the resolution of the computed mosaic by capturing 360 degree slices rather than strips. A variety of slice cameras are proposed that map a thin 360 degree sheet of rays onto a large image area. This results in the capture of high resolution slices despite the use of a low resolution video camera. A slice camera is rotated using a motorized turntable to obtain regular as well as stereoscopic spherical mosaics.",
            "magazine": "CVPR"
        },
        {
            "paperId": 2719,
            "title": "Human detection using depth information by Kinect",
            "link": "https://doi.org/10.1109/CVPRW.2011.5981811",
            "abstracts": "Conventional human detection is mostly done in images taken by visible-light cameras. These methods imitate the detection process that human use. They use features based on gradients, such as histograms of oriented gradients (HOG), or extract interest points in the image, such as scale-invariant feature transform (SIFT), etc. In this paper, we present a novel human detection method using depth information taken by the Kinect for Xbox 360. We propose a model based approach, which detects humans using a 2-D head contour model and a 3-D head surface model. We propose a segmentation scheme to segment the human from his/her surroundings and extract the whole contours of the figure based on our detection point. We also explore the tracking algorithm based on our detection result. The methods are tested on our database taken by the Kinect in our lab and present superior results.",
            "magazine": "CVPR"
        },
        {
            "paperId": 3779,
            "title": "A Novel HDR Depth Camera for Real-Time 3D 360° Panoramic Vision",
            "link": "https://doi.org/10.1109/CVPRW.2014.69",
            "abstracts": "This paper presents a novel 360° High-Dynamic Range (HDR) camera for real-time 3D 360° panoramic computer vision. The camera consists of (1) a pair of bio-inspired dynamic vision line sensors (1024 pixel each) asynchronously generating events at high temporal resolution with on-chip time stamping (1μs resolution), having a high dynamic range and the sparse visual coding of the information, (2) a high-speed mechanical device rotating at up to 10 revolutions per sec (rps) on which the pair of sensor is mounted and (3) a processing unit for the configuration of the detector chip and transmission of its data through a slip ring and a gigabit Ethernet communication to the user. Within this work, we first present the new camera, its individual components and resulting panoramic edge map. In a second step, we developed a method for reconstructing the intensity images out of the event data generated by the sensors. The algorithm maps the recorded panoramic views into gray-level images by using a transform coefficient. In the last part of this work, anaglyph representation and 3D reconstruction results out of the stereo images are shown. The experimental results show the capabilities of the new camera to generate 10 x 3D panoramic views per second in real-time at an image resolution of 5000x1024 pixel and intra-scene dynamic range of more than 120 dB under natural illuminations. The camera potential for 360° depth imaging and mobile computer vision is briefly highlighted.",
            "magazine": "CVPR"
        },
        {
            "paperId": 3779,
            "title": "A Novel HDR Depth Camera for Real-Time 3D 360° Panoramic Vision",
            "link": "https://doi.org/10.1109/CVPRW.2014.69",
            "abstracts": "This paper presents a novel 360° High-Dynamic Range (HDR) camera for real-time 3D 360° panoramic computer vision. The camera consists of (1) a pair of bio-inspired dynamic vision line sensors (1024 pixel each) asynchronously generating events at high temporal resolution with on-chip time stamping (1μs resolution), having a high dynamic range and the sparse visual coding of the information, (2) a high-speed mechanical device rotating at up to 10 revolutions per sec (rps) on which the pair of sensor is mounted and (3) a processing unit for the configuration of the detector chip and transmission of its data through a slip ring and a gigabit Ethernet communication to the user. Within this work, we first present the new camera, its individual components and resulting panoramic edge map. In a second step, we developed a method for reconstructing the intensity images out of the event data generated by the sensors. The algorithm maps the recorded panoramic views into gray-level images by using a transform coefficient. In the last part of this work, anaglyph representation and 3D reconstruction results out of the stereo images are shown. The experimental results show the capabilities of the new camera to generate 10 x 3D panoramic views per second in real-time at an image resolution of 5000x1024 pixel and intra-scene dynamic range of more than 120 dB under natural illuminations. The camera potential for 360° depth imaging and mobile computer vision is briefly highlighted.",
            "magazine": "CVPR"
        },
        {
            "paperId": 3779,
            "title": "A Novel HDR Depth Camera for Real-Time 3D 360° Panoramic Vision",
            "link": "https://doi.org/10.1109/CVPRW.2014.69",
            "abstracts": "This paper presents a novel 360° High-Dynamic Range (HDR) camera for real-time 3D 360° panoramic computer vision. The camera consists of (1) a pair of bio-inspired dynamic vision line sensors (1024 pixel each) asynchronously generating events at high temporal resolution with on-chip time stamping (1μs resolution), having a high dynamic range and the sparse visual coding of the information, (2) a high-speed mechanical device rotating at up to 10 revolutions per sec (rps) on which the pair of sensor is mounted and (3) a processing unit for the configuration of the detector chip and transmission of its data through a slip ring and a gigabit Ethernet communication to the user. Within this work, we first present the new camera, its individual components and resulting panoramic edge map. In a second step, we developed a method for reconstructing the intensity images out of the event data generated by the sensors. The algorithm maps the recorded panoramic views into gray-level images by using a transform coefficient. In the last part of this work, anaglyph representation and 3D reconstruction results out of the stereo images are shown. The experimental results show the capabilities of the new camera to generate 10 x 3D panoramic views per second in real-time at an image resolution of 5000x1024 pixel and intra-scene dynamic range of more than 120 dB under natural illuminations. The camera potential for 360° depth imaging and mobile computer vision is briefly highlighted.",
            "magazine": "CVPR"
        },
        {
            "paperId": 3779,
            "title": "A Novel HDR Depth Camera for Real-Time 3D 360° Panoramic Vision",
            "link": "https://doi.org/10.1109/CVPRW.2014.69",
            "abstracts": "This paper presents a novel 360° High-Dynamic Range (HDR) camera for real-time 3D 360° panoramic computer vision. The camera consists of (1) a pair of bio-inspired dynamic vision line sensors (1024 pixel each) asynchronously generating events at high temporal resolution with on-chip time stamping (1μs resolution), having a high dynamic range and the sparse visual coding of the information, (2) a high-speed mechanical device rotating at up to 10 revolutions per sec (rps) on which the pair of sensor is mounted and (3) a processing unit for the configuration of the detector chip and transmission of its data through a slip ring and a gigabit Ethernet communication to the user. Within this work, we first present the new camera, its individual components and resulting panoramic edge map. In a second step, we developed a method for reconstructing the intensity images out of the event data generated by the sensors. The algorithm maps the recorded panoramic views into gray-level images by using a transform coefficient. In the last part of this work, anaglyph representation and 3D reconstruction results out of the stereo images are shown. The experimental results show the capabilities of the new camera to generate 10 x 3D panoramic views per second in real-time at an image resolution of 5000x1024 pixel and intra-scene dynamic range of more than 120 dB under natural illuminations. The camera potential for 360° depth imaging and mobile computer vision is briefly highlighted.",
            "magazine": "CVPR"
        },
        {
            "paperId": 4090,
            "title": "Spectral-360: A Physics-Based Technique for Change Detection",
            "link": "https://doi.org/10.1109/CVPRW.2014.65",
            "abstracts": "This paper presents and assesses a novel physics-based change detection technique, Spectral-360, which is based on the dichromatic color reflectance model. This approach, uses image formation models to computationally estimate, from the camera output, a consistent physics-based color descriptor of the spectral reflectance of surfaces visible in the image, and then to measure the similarity between the full-spectrum reflectance of the background and foreground pixels to segment the foreground from a static background. This method represents a new approach to change detection, using explicit hypotheses about the physics that create images. The assumptions which have been made are that diffuse-only-reflection is applicable, and the existence of a dominant illuminant. The objective evaluation performed using the 'changedetection.net 2014' dataset shows that our Spectral-360 method outperforms most state-of-the-art methods.",
            "magazine": "CVPR"
        },
        {
            "paperId": 4291,
            "title": "Event-driven stereo matching for real-time 3D panoramic vision",
            "link": "https://doi.org/10.1109/CVPR.2015.7298644",
            "abstracts": "This paper presents a stereo matching approach for a novel multi-perspective panoramic stereo vision system, making use of asynchronous and non-simultaneous stereo imaging towards real-time 3D 360° vision. The method is designed for events representing the scenes visual contrast as a sparse visual code allowing the stereo reconstruction of high resolution panoramic views. We propose a novel cost measure for the stereo matching, which makes use of a similarity measure based on event distributions. Thus, the robustness to variations in event occurrences was increased. An evaluation of the proposed stereo method is presented using distance estimation of panoramic stereo views and ground truth data. Furthermore, our approach is compared to standard stereo methods applied on event-data. Results show that we obtain 3D reconstructions of 1024 × 3600 round views and outperform depth reconstruction accuracy of state-of-the-art methods on event data.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5092,
            "title": "Deep 360 Pilot: Learning a Deep Agent for Piloting through 360° Sports Videos",
            "link": "https://doi.org/10.1109/CVPR.2017.153",
            "abstracts": "Watching a 360° sports video requires a viewer to continuously select a viewing angle, either through a sequence of mouse clicks or head movements. To relieve the viewer from this “360 piloting” task, we propose “deep 360 pilot” - a deep learning-based agent for piloting through 360° sports videos automatically. At each frame, the agent observes a panoramic image and has the knowledge of previously selected viewing angles. The task of the agent is to shift the current viewing angle (i.e. action) to the next preferred one (i.e., goal). We propose to directly learn an online policy of the agent from data. Specifically, we leverage a state-of-the-art object detector to propose a few candidate objects of interest (yellow boxes in Fig. 1). Then, a recurrent neural network is used to select the main object (green dash boxes in Fig. 1). Given the main object and previously selected viewing angles, our method regresses a shift in viewing angle to move to the next one. We use the policy gradient technique to jointly train our pipeline, by minimizing: (1) a regression loss measuring the distance between the selected and ground truth viewing angles, (2) a smoothness loss encouraging smooth transition in viewing angle, and (3) maximizing an expected reward offocusing on a foreground object. To evaluate our method, we built a new 360-Sports video dataset consisting offive sports domains. We trained domain-specific agents and achieved the best performance on viewing angle selection accuracy and users' preference compared to [53] and other baselines.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5092,
            "title": "Deep 360 Pilot: Learning a Deep Agent for Piloting through 360° Sports Videos",
            "link": "https://doi.org/10.1109/CVPR.2017.153",
            "abstracts": "Watching a 360° sports video requires a viewer to continuously select a viewing angle, either through a sequence of mouse clicks or head movements. To relieve the viewer from this “360 piloting” task, we propose “deep 360 pilot” - a deep learning-based agent for piloting through 360° sports videos automatically. At each frame, the agent observes a panoramic image and has the knowledge of previously selected viewing angles. The task of the agent is to shift the current viewing angle (i.e. action) to the next preferred one (i.e., goal). We propose to directly learn an online policy of the agent from data. Specifically, we leverage a state-of-the-art object detector to propose a few candidate objects of interest (yellow boxes in Fig. 1). Then, a recurrent neural network is used to select the main object (green dash boxes in Fig. 1). Given the main object and previously selected viewing angles, our method regresses a shift in viewing angle to move to the next one. We use the policy gradient technique to jointly train our pipeline, by minimizing: (1) a regression loss measuring the distance between the selected and ground truth viewing angles, (2) a smoothness loss encouraging smooth transition in viewing angle, and (3) maximizing an expected reward offocusing on a foreground object. To evaluate our method, we built a new 360-Sports video dataset consisting offive sports domains. We trained domain-specific agents and achieved the best performance on viewing angle selection accuracy and users' preference compared to [53] and other baselines.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5092,
            "title": "Deep 360 Pilot: Learning a Deep Agent for Piloting through 360° Sports Videos",
            "link": "https://doi.org/10.1109/CVPR.2017.153",
            "abstracts": "Watching a 360° sports video requires a viewer to continuously select a viewing angle, either through a sequence of mouse clicks or head movements. To relieve the viewer from this “360 piloting” task, we propose “deep 360 pilot” - a deep learning-based agent for piloting through 360° sports videos automatically. At each frame, the agent observes a panoramic image and has the knowledge of previously selected viewing angles. The task of the agent is to shift the current viewing angle (i.e. action) to the next preferred one (i.e., goal). We propose to directly learn an online policy of the agent from data. Specifically, we leverage a state-of-the-art object detector to propose a few candidate objects of interest (yellow boxes in Fig. 1). Then, a recurrent neural network is used to select the main object (green dash boxes in Fig. 1). Given the main object and previously selected viewing angles, our method regresses a shift in viewing angle to move to the next one. We use the policy gradient technique to jointly train our pipeline, by minimizing: (1) a regression loss measuring the distance between the selected and ground truth viewing angles, (2) a smoothness loss encouraging smooth transition in viewing angle, and (3) maximizing an expected reward offocusing on a foreground object. To evaluate our method, we built a new 360-Sports video dataset consisting offive sports domains. We trained domain-specific agents and achieved the best performance on viewing angle selection accuracy and users' preference compared to [53] and other baselines.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5297,
            "title": "Making 360° Video Watchable in 2D: Learning Videography for Click Free Viewing",
            "link": "https://doi.org/10.1109/CVPR.2017.150",
            "abstracts": "360° Video requires human viewers to actively control where to look while watching the video. Although it provides a more immersive experience of the visual content, it also introduces additional burden for viewers, awkward interfaces to navigate the video lead to suboptimal viewing experiences. Virtual cinematography is an appealing direction to remedy these problems, but conventional methods are limited to virtual environments or rely on hand-crafted heuristics. We propose a new algorithm for virtual cinematography that automatically controls a virtual camera within a 360° video. Compared to the state of the art, our algorithm allows more general camera control, avoids redundant outputs, and extracts its output videos substantially more efficiently. Experimental results on over 7 hours of real in the wild video show that our generalized camera control is crucial for viewing 360° video, while the proposed efficient algorithm is essential for making the generalized control computationally tractable.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5509,
            "title": "A Memory Network Approach for Story-Based Temporal Summarization of 360° Videos",
            "link": "https://doi.org/10.1109/CVPR.2018.00153",
            "abstracts": "We address the problem of story-based temporal summarization of long 360° videos. We propose a novel memory network model named Past-Future Memory Network (PFMN), in which we first compute the scores of 81 normal field of view (NFOV) region proposals cropped from the input 360° video, and then recover a latent, collective summary using the network with two external memories that store the embeddings of previously selected subshots and future candidate subshots. Our major contributions are twofold. First, our work is the first to address story-based temporal summarization of 360° videos. Second, our model is the first attempt to leverage memory networks for video summarization tasks. For evaluation, we perform three sets of experiments. First, we investigate the view selection capability of our model on the Pano2Vid dataset [42]. Second, we evaluate the temporal summarization with a newly collected 360° video dataset. Finally, we experiment our model's performance in another domain, with image-based storytelling VIST dataset [22]. We verify that our model achieves state-of-the-art performance on all the tasks.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5569,
            "title": "Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos",
            "link": "https://doi.org/10.1109/CVPR.2018.00154",
            "abstracts": "Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) weakly-supervised trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, CP introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms baseline methods in both speed and quality.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5569,
            "title": "Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos",
            "link": "https://doi.org/10.1109/CVPR.2018.00154",
            "abstracts": "Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) weakly-supervised trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, CP introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms baseline methods in both speed and quality.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5569,
            "title": "Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos",
            "link": "https://doi.org/10.1109/CVPR.2018.00154",
            "abstracts": "Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) weakly-supervised trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, CP introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms baseline methods in both speed and quality.",
            "magazine": "CVPR"
        },
        {
            "paperId": 5569,
            "title": "Cube Padding for Weakly-Supervised Saliency Prediction in 360° Videos",
            "link": "https://doi.org/10.1109/CVPR.2018.00154",
            "abstracts": "Automatic saliency prediction in 360° videos is critical for viewpoint guidance applications (e.g., Facebook 360 Guide). We propose a spatial-temporal network which is (1) weakly-supervised trained and (2) tailor-made for 360° viewing sphere. Note that most existing methods are less scalable since they rely on annotated saliency map for training. Most importantly, they convert 360° sphere to 2D images (e.g., a single equirectangular image or multiple separate Normal Field-of-View (NFoV) images) which introduces distortion and image boundaries. In contrast, we propose a simple and effective Cube Padding (CP) technique as follows. Firstly, we render the 360° view on six faces of a cube using perspective projection. Thus, it introduces very little distortion. Then, we concatenate all six faces while utilizing the connectivity between faces on the cube for image padding (i.e., Cube Padding) in convolution, pooling, convolutional LSTM layers. In this way, CP introduces no image boundary while being applicable to almost all Convolutional Neural Network (CNN) structures. To evaluate our method, we propose Wild-360, a new 360° video saliency dataset, containing challenging videos with saliency heatmap annotations. In experiments, our method outperforms baseline methods in both speed and quality.",
            "magazine": "CVPR"
        },
        {
            "paperId": 6465,
            "title": "BiFuse: Monocular 360 Depth Estimation via Bi-Projection Fusion",
            "link": "https://doi.org/10.1109/CVPR42600.2020.00054",
            "abstracts": "Depth estimation from a monocular 360 image is an emerging problem that gains popularity due to the availability of consumer-level 360 cameras and the complete surrounding sensing capability. While the standard of 360 imaging is under rapid development, we propose to predict the depth map of a monocular 360 image by mimicking both peripheral and foveal vision of the human eye. To this end, we adopt a two-branch neural network leveraging two common projections: equirectangular and cubemap projections. In particular, equirectangular projection incorporates a complete field-of-view but introduces distortion, whereas cubemap projection avoids distortion but introduces discontinuity at the boundary of the cube. Thus we propose a bi-projection fusion scheme along with learnable masks to balance the feature map from the two projections. Moreover, for the cubemap projection, we propose a spherical padding procedure which mitigates discontinuity at the boundary of each face. We apply our method to four panorama datasets and show favorable results against the existing state-of-the-art methods.",
            "magazine": "CVPR"
        },
        {
            "paperId": 6465,
            "title": "BiFuse: Monocular 360 Depth Estimation via Bi-Projection Fusion",
            "link": "https://doi.org/10.1109/CVPR42600.2020.00054",
            "abstracts": "Depth estimation from a monocular 360 image is an emerging problem that gains popularity due to the availability of consumer-level 360 cameras and the complete surrounding sensing capability. While the standard of 360 imaging is under rapid development, we propose to predict the depth map of a monocular 360 image by mimicking both peripheral and foveal vision of the human eye. To this end, we adopt a two-branch neural network leveraging two common projections: equirectangular and cubemap projections. In particular, equirectangular projection incorporates a complete field-of-view but introduces distortion, whereas cubemap projection avoids distortion but introduces discontinuity at the boundary of the cube. Thus we propose a bi-projection fusion scheme along with learnable masks to balance the feature map from the two projections. Moreover, for the cubemap projection, we propose a spherical padding procedure which mitigates discontinuity at the boundary of each face. We apply our method to four panorama datasets and show favorable results against the existing state-of-the-art methods.",
            "magazine": "CVPR"
        },
        {
            "paperId": 6465,
            "title": "BiFuse: Monocular 360 Depth Estimation via Bi-Projection Fusion",
            "link": "https://doi.org/10.1109/CVPR42600.2020.00054",
            "abstracts": "Depth estimation from a monocular 360 image is an emerging problem that gains popularity due to the availability of consumer-level 360 cameras and the complete surrounding sensing capability. While the standard of 360 imaging is under rapid development, we propose to predict the depth map of a monocular 360 image by mimicking both peripheral and foveal vision of the human eye. To this end, we adopt a two-branch neural network leveraging two common projections: equirectangular and cubemap projections. In particular, equirectangular projection incorporates a complete field-of-view but introduces distortion, whereas cubemap projection avoids distortion but introduces discontinuity at the boundary of the cube. Thus we propose a bi-projection fusion scheme along with learnable masks to balance the feature map from the two projections. Moreover, for the cubemap projection, we propose a spherical padding procedure which mitigates discontinuity at the boundary of each face. We apply our method to four panorama datasets and show favorable results against the existing state-of-the-art methods.",
            "magazine": "CVPR"
        },
        {
            "paperId": 6604,
            "title": "Geometric Structure Based and Regularized Depth Estimation From 360 Indoor Imagery",
            "link": "https://doi.org/10.1109/CVPR42600.2020.00097",
            "abstracts": "Motivated by the correlation between the depth and the geometric structure of a 360 indoor image, we propose a novel learning-based depth estimation framework that leverages the geometric structure of a scene to conduct depth estimation. Specifically, we represent the geometric structure of an indoor scene as a collection of corners, boundaries and planes. On the one hand, once a depth map is estimated, this geometric structure can be inferred from the estimated depth map; thus, the geometric structure functions as a regularizer for depth estimation. On the other hand, this estimation also benefits from the geometric structure of a scene estimated from an image where the structure functions as a prior. However, furniture in indoor scenes makes it challenging to infer geometric structure from depth or image data. An attention map is inferred to facilitate both depth estimation from features of the geometric structure and also geometric inferences from the estimated depth map. To validate the effectiveness of each component in our framework under controlled conditions, we render a synthetic dataset, Shanghaitech-Kujiale Indoor 360 dataset with 3550 360 indoor images. Extensive experiments on popular datasets validate the effectiveness of our solution. We also demonstrate that our method can also be applied to counterfactual depth.",
            "magazine": "CVPR"
        },
        {
            "paperId": 13452,
            "title": "Eliminating the Blind Spot: Adapting 3D Object Detection and Monocular Depth Estimation to 360(^circ ) Panoramic Imagery",
            "link": "https://doi.org/10.1007/978-3-030-01261-8_48",
            "abstracts": "Recent automotive vision work has focused almost exclusively on processing forward-facing cameras. However, future autonomous vehicles will not be viable without a more comprehensive surround sensing, akin to a human driver, as can be provided by 360(^circ )panoramic cameras. We present an approach to adapt contemporary deep network architectures developed on conventional rectilinear imagery to work on equirectangular 360(^circ ) panoramic imagery. To address the lack of annotated panoramic automotive datasets availability, we adapt contemporary automotive dataset, via style and projection transformations, to facilitate the cross-domain retraining of contemporary algorithms for panoramic imagery. Following this approach we retrain and adapt existing architectures to recover scene depth and 3D pose of vehicles from monocular panoramic imagery without any panoramic training labels or calibration parameters. Our approach is evaluated qualitatively on crowd-sourced panoramic images and quantitatively using an automotive environment simulator to provide the first benchmark for such techniques within panoramic imagery.",
            "magazine": "ECCV"
        },
        {
            "paperId": 13822,
            "title": "OmniDepth: Dense Depth Estimation for Indoors Spherical Panoramas",
            "link": "https://doi.org/10.1007/978-3-030-01231-1_28",
            "abstracts": "Recent work on depth estimation up to now has only focused on projective images ignoring ({360}^{circ }) content which is now increasingly and more easily produced. We show that monocular depth estimation models trained on traditional images produce sub-optimal results on omnidirectional images, showcasing the need for training directly on ({360}^{circ }) datasets, which however, are hard to acquire. In this work, we circumvent the challenges associated with acquiring high quality ({360}^{circ }) datasets with ground truth depth annotations, by re-using recently released large scale 3D datasets and re-purposing them to ({360}^{circ }) via rendering. This dataset, which is considerably larger than similar projective datasets, is publicly offered to the community to enable future research in this direction. We use this dataset to learn in an end-to-end fashion the task of depth estimation from ({360}^{circ }) images. We show promising results in our synthesized data as well as in unseen realistic images.",
            "magazine": "ECCV"
        },
        {
            "paperId": 13968,
            "title": "Saliency Detection in 360(^circ ) Videos",
            "link": "https://doi.org/10.1007/978-3-030-01234-2_30",
            "abstracts": "This paper presents a novel spherical convolutional neural network based scheme for saliency detection for (360^circ ) videos. Specifically, in our spherical convolution neural network definition, kernel is defined on a spherical crown, and the convolution involves the rotation of the kernel along the sphere. Considering that the (360^circ ) videos are usually stored with equirectangular panorama, we propose to implement the spherical convolution on panorama by stretching and rotating the kernel based on the location of patch to be convolved. Compared with existing spherical convolution, our definition has the parameter sharing property, which would greatly reduce the parameters to be learned. We further take the temporal coherence of the viewing process into consideration, and propose a sequential saliency detection by leveraging a spherical U-Net. To validate our approach, we construct a large-scale (360^circ ) videos saliency detection benchmark that consists of 104 (360^circ ) videos viewed by 20+ human subjects. Comprehensive experiments validate the effectiveness of our spherical U-net for (360^circ ) video saliency detection.",
            "magazine": "ECCV"
        },
        {
            "paperId": 14032,
            "title": "Snap Angle Prediction for 360(^{circ }) Panoramas",
            "link": "https://doi.org/10.1007/978-3-030-01228-1_1",
            "abstracts": "360(^{circ }) panoramas are a rich medium, yet notoriously difficult to visualize in the 2D image plane. We explore how intelligent rotations of a spherical image may enable content-aware projection with fewer perceptible distortions. Whereas existing approaches assume the viewpoint is fixed, intuitively some viewing angles within the sphere preserve high-level objects better than others. To discover the relationship between these optimal snap angles and the spherical panorama’s content, we develop a reinforcement learning approach for the cubemap projection model. Implemented as a deep recurrent neural network, our method selects a sequence of rotation actions and receives reward for avoiding cube boundaries that overlap with important foreground objects. We show our approach creates more visually pleasing panoramas while using 5x less computation than the baseline.",
            "magazine": "ECCV"
        },
        {
            "paperId": 14346,
            "title": "AtlantaNet: Inferring the 3D Indoor Layout from a Single (360^circ ) Image Beyond the Manhattan World Assumption",
            "link": "https://doi.org/10.1007/978-3-030-58598-3_26",
            "abstracts": "We introduce a novel end-to-end approach to predict a 3D room layout from a single panoramic image. Compared to recent state-of-the-art works, our method is not limited to Manhattan World environments, and can reconstruct rooms bounded by vertical walls that do not form right angles or are curved – i.e., Atlanta World models. In our approach, we project the original gravity-aligned panoramic image on two horizontal planes, one above and one below the camera. This representation encodes all the information needed to recover the Atlanta World 3D bounding surfaces of the room in the form of a 2D room footprint on the floor plan and a room height. To predict the 3D layout, we propose an encoder-decoder neural network architecture, leveraging Recurrent Neural Networks (RNNs) to capture long-range geometric patterns, and exploiting a customized training strategy based on domain-specific knowledge. The experimental results demonstrate that our method outperforms state-of-the-art solutions in prediction accuracy, in particular in cases of complex wall layouts or curved wall footprints.",
            "magazine": "ECCV"
        },
        {
            "paperId": 14552,
            "title": "Deep Multi Depth Panoramas for View Synthesis",
            "link": "https://doi.org/10.1007/978-3-030-58601-0_20",
            "abstracts": "We propose a learning-based approach for novel view synthesis for multi-camera 360(^circ ) panorama capture rigs. Previous work constructs RGBD panoramas from such data, allowing for view synthesis with small amounts of translation, but cannot handle the disocclusions and view-dependent effects that are caused by large translations. To address this issue, we present a novel scene representation—Multi Depth Panorama (MDP)—that consists of multiple RGBD(alpha ) panoramas that represent both scene geometry and appearance. We demonstrate a deep neural network-based method to reconstruct MDPs from multi-camera 360(^circ ) images. MDPs are more compact than previous 3D scene representations and enable high-quality, efficient new view rendering. We demonstrate this via experiments on both synthetic and real data and comparisons with previous state-of-the-art methods spanning both learning-based approaches and classical RGBD-based methods.",
            "magazine": "ECCV"
        },
        {
            "paperId": 15398,
            "title": "Real-Time Detection of Multiple Targets from a Moving 360(^{circ }) Panoramic Imager in the Wild",
            "link": "https://doi.org/10.1007/978-3-030-68238-5_8",
            "abstracts": "Our goal is to develop embedded and mobile vision applications leveraging state-of-the-art visual sensors and efficient neural network architectures deployed on emerging neural computing engines for smart monitoring and inspection purposes. In this paper, we present 360(^{circ }) vision system onboard an automobile or UAV platform for large field-of-view and real-time detection of multiple challenging objects. The targeted objects include flag as a deformable object; UAV as a tiny, flying object which changes its scales and positions rapidly; and grouped objects containing piled sandbags as deformable objects in a group themselves, flag and stop sign to form a scene representing an artificial fake checkpoint. Barrel distortions owing to the 360(^{circ }) optics make the detection task even more challenging. A light-weight neural network model based on MobileNets architecture is transfer learned for detection of the custom objects with very limited training data. In method 1, we generated a dataset of perspective planar images via a virtual camera model which projects a patch on the hemisphere to a 2D plane. In method 2, the panomorph images are directly used without projection. Real-time detection of the objects in 360(^{circ }) video is realized by feeding live streamed frames captured by the full hemispheric (180(^{circ }) (\times ) 360(^{circ })) field-of-view ImmerVision Enables panomorph lens to the trained MobileNets model. We found that with only few training data which is far less than 10 times of Vapnik–Chervonenkis dimension of the model, the MobileNets model achieves a detection rate of 80–90% for test data having a similar distribution as the training data. However, the model performance dropped drastically when it was put in action in the wild for unknown data in which both weather and lighting conditions were different. The generalization capability of the model can be improved by training with more data. The contribution of this work is a 360(^{circ }) vision hardware and software system for real-time detection of challenging objects. This system could be configured for very low-power embedded applications by running inferences via a neural computing engine such as Intel Movidius NSC2 or HiSilicon Kirin 970.",
            "magazine": "ECCV"
        }
    ]
}